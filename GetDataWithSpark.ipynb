{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This note book load date from h5 file and save them to csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO :\n",
    "- use Spark to have more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import h5py\n",
    "import hdf5_getters\n",
    "from hdf5_getters import *\n",
    "\n",
    "from operator import methodcaller\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_all_files(base_directory, ext='.h5') :\n",
    "    cnt = 0\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        files = glob.glob(os.path.join(root,'*'+ext))\n",
    "        cnt += len(files)\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all(getter, base_directory, ext='.h5'):\n",
    "    values = []\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        files = glob.glob(os.path.join(root,'*'+ext))\n",
    "        for f in files:\n",
    "            h5 = hdf5_getters.open_h5_file_read(f)\n",
    "            values.append( getter(h5) )\n",
    "            h5.close()\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_fields(file, all_getters):\n",
    "    fields = []\n",
    "    for getter_name in all_getters:\n",
    "        getter = getattr(hdf5_getters, getter_name)\n",
    "        fields.append(getter(file))\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/benjamin/./millionsongsubset_full/MillionSongSubset/data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_directory = './millionsongsubset_full/MillionSongSubset/data'\n",
    "import getpass\n",
    "if getpass.getuser() == \"benjamin\":\n",
    "    base_directory = \"/home/benjamin/\" + base_directory\n",
    "base_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get_mode', 'get_bars_confidence', 'get_artist_familiarity', 'get_year', 'get_analysis_sample_rate', 'get_beats_start', 'get_artist_latitude', 'get_artist_id', 'get_segments_confidence', 'get_bars_start', 'get_track_7digitalid', 'get_artist_name', 'get_similar_artists', 'get_key_confidence', 'get_danceability', 'get_tempo', 'get_segments_timbre', 'get_song_id', 'get_artist_mbid', 'get_end_of_fade_in', 'get_time_signature_confidence', 'get_artist_playmeid', 'get_segments_start', 'get_loudness', 'get_artist_terms_freq', 'get_tatums_start', 'get_time_signature', 'get_artist_location', 'get_tatums_confidence', 'get_beats_confidence', 'get_audio_md5', 'get_title', 'get_artist_7digitalid', 'get_song_hotttnesss', 'get_artist_hotttnesss', 'get_artist_mbtags', 'get_release_7digitalid', 'get_num_songs', 'get_artist_mbtags_count', 'get_release', 'get_artist_terms_weight', 'get_segments_loudness_max', 'get_artist_longitude', 'get_duration', 'get_segments_pitches', 'get_artist_terms', 'get_mode_confidence', 'get_start_of_fade_out', 'get_segments_loudness_max_time', 'get_segments_loudness_start', 'get_sections_confidence', 'get_key', 'get_sections_start', 'get_track_id', 'get_energy']\n",
      "Number of getters: 55\n"
     ]
    }
   ],
   "source": [
    "all_getters = list(filter(lambda x: x[:3] == 'get',hdf5_getters.__dict__.keys()))\n",
    "print(all_getters)\n",
    "print('Number of getters:', len(all_getters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the dataframe (only subset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over all the .h5 files, starting from the base directory\n",
    "#note: takes between ~8 to ~10 minutes\n",
    "start_time = time.time()\n",
    "\n",
    "values = []\n",
    "ext = '.h5'\n",
    "i = 0\n",
    "for root, dirs, files in os.walk(base_directory):\n",
    "    files = glob.glob(os.path.join(root, '*'+ext))\n",
    "    for f in files:\n",
    "        h5 = hdf5_getters.open_h5_file_read(f)\n",
    "        values.append(get_file_fields(h5, all_getters))\n",
    "        h5.close()\n",
    "        if(i%100==0):\n",
    "            print(i*1.0/nbr_files*100,\"% chargée\")\n",
    "        i = i+1\n",
    "\n",
    "print(\"--- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [name[4:].replace('_', ' ') for name in all_getters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe with the given column names\n",
    "data = pd.DataFrame(columns=column_names)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the dataframe row by row\n",
    "start_time = time.time()\n",
    "nb_entries = len(values)\n",
    "for i in range(0, nb_entries):\n",
    "    if(i%100==0):\n",
    "            print(i*1.0/nb_entries*100,\"% chargée\")\n",
    "    data.loc[i] = values[i]\n",
    "print(\"--- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data, filename, folder='./data/'):\n",
    "    #note: newline='' prevents Python 3 from adding a new line after writing each row\n",
    "    with open(folder+filename, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        if(type(data[0]) == np.ndarray):\n",
    "            #unfold the pandas series into an array of array\n",
    "            array = [subarray for subarray in data]\n",
    "            writer.writerows(array)\n",
    "        else:\n",
    "            writer.writerow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing the dataframe to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [name[4:].replace('_', ' ') for name in all_getters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "start_time = time.time()\n",
    "for column_name in column_names:\n",
    "    count += 1\n",
    "    if column_name not in [\"segments pitches\",\"segments timbre\"] :\n",
    "        print(count, column_name)\n",
    "        save_to_csv(data[column_name], column_name.replace(' ', '_')+'.csv')\n",
    "    \n",
    "print(\"--- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/benjamin/./millionsongsubset_full/MillionSongSubset/data'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tables\n",
    "\n",
    "path  = base_directory+\"/A/A/A/TRAAAAW128F429D538.h5\"\n",
    "content = open(path, \"rb\").read()\n",
    "\n",
    "#h5 = hdf5_getters.open_h5_file_read()\n",
    "#h5\n",
    "\n",
    "\n",
    "def LoadData(content):\n",
    "    h5 = tables.open_file(\"in-memory-sample.h5\", driver=\"H5FD_CORE\",\n",
    "                              driver_core_image=content,\n",
    "                              driver_core_backing_store=0)\n",
    "\n",
    "    data = [get_year(h5),get_artist_terms(h5),get_artist_latitude(h5),get_artist_longitude(h5)]\n",
    "    \n",
    "    \n",
    "    return data\n",
    "\n",
    "def Filter(data):\n",
    "        print(np.isnan(data[2]))\n",
    "        return (data[0]!=0 and  not np.isnan(data[2]))\n",
    "        \n",
    "Filter(LoadData(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.bytes_"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(LoadData(content)[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0, [\"hip hop\", \"underground rap\", \"g funk\", \"alternative rap\", \"gothic rock\", \"west coast rap\", \"rap\", \"club dance\", \"singer-songwriter\", \"chill-out\", \"underground hip hop\", \"rock\", \"gothic\", \"san francisco bay area\", \"indie\", \"american\", \"punk\", \"california\", \"industrial\", \"new york\", \"90s\", \"latin\", \"spanish\", \"dark\", \"ebm\", \"underground\", \"deathrock\", \"west coast\", \"san francisco\", \"producer\", \"oakland\", \"catalan\", \"barcelona\", \"doomsdope\", \"norcal\", \"west coast hip hop\", \"alternative rock\"], NaN, NaN],\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "class NumpyAwareJSONEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray) and obj.ndim == 1:\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, np.int32):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.bytes_) or isinstance(obj, bytes):\n",
    "            return obj.decode(\"utf-8\")\n",
    "        print(type(obj))\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "def jsonData(data):\n",
    "    return json.dumps(data,cls=NumpyAwareJSONEncoder)+',\\n'\n",
    "\n",
    "jsonData(LoadData(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.io.EOFException: End of File Exception between local host is: \"Benjamin-Surface/127.0.1.1\"; destination host is: \"iccluster060.iccluster.epfl.ch\":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1472)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1399)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n\tat com.sun.proxy.$Proxy23.updateBlockForPipeline(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolTranslatorPB.java:877)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy24.updateBlockForPipeline(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1266)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1004)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:548)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-0f9f755f0108>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET DATA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yarn-client\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddPyFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./hdf5_getters.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-1.6.3-bin-hadoop2.6/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m--> 115\u001b[0;31m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-1.6.3-bin-hadoop2.6/python/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# Create a single Accumulator in Java that we'll send all our updates through;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-1.6.3-bin-hadoop2.6/python/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \"\"\"\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-1.6.3-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1064\u001b[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-1.6.3-bin-hadoop2.6/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.io.EOFException: End of File Exception between local host is: \"Benjamin-Surface/127.0.1.1\"; destination host is: \"iccluster060.iccluster.epfl.ch\":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1472)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1399)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n\tat com.sun.proxy.$Proxy23.updateBlockForPipeline(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolTranslatorPB.java:877)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy24.updateBlockForPipeline(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1266)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1004)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:548)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readInt(DataInputStream.java:392)\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1071)\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:966)\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"/home/benjamin/spark/spark-1.6.3-bin-hadoop2.6\")\n",
    "import pyspark \n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf.setAppName(\"GET DATA\")\n",
    "conf.setMaster(\"yarn\")\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "sc.addPyFile(\"./hdf5_getters.py\")\n",
    "\n",
    "path='hdfs:/datasets/million-song_untar/A/A/A/*.h5'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sc.binaryFiles(path).map(lambda path,file : LoadData(file) ).filter(Filter).map(jsonData).saveAsTextFile(\"data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
